<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reproducability on </title>
    <link>http://localhost:1313/categories/reproducability/</link>
    <description>Recent content in Reproducability on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 26 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/reproducability/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Last Post: Logging Datasets in Machine Learning Experiments with MLflow</title>
      <link>http://localhost:1313/blog/markdown_syntax/</link>
      <pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/markdown_syntax/</guid>
      <description>&lt;p&gt;MLflow is widely recognized as a powerful tool for tracking machine learning (ML) experiments, enabling data scientists and ML experts to systematically log metrics, parameters, and models. However, its capabilities extend beyond these traditional use cases â€” MLflow can also be used to track datasets. Logging information about training and evaluation datasets is a critical step in enhancing the transparency and reproducibility of ML experiments. By capturing dataset details, such as versioning and splits, MLflow helps ensure that experiments can be accurately replicated and understood.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
